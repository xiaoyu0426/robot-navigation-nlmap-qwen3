#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Qwen3-4B æœ¬åœ°éƒ¨ç½²æµ‹è¯•è„šæœ¬
éªŒè¯æ¨¡å‹æ˜¯å¦èƒ½å¤ŸæˆåŠŸåŠ è½½å’Œè¿è¡Œ
"""

import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import os

def test_qwen3_deployment():
    """
    æµ‹è¯•Qwen3-4Bæ¨¡å‹çš„æœ¬åœ°éƒ¨ç½²
    """
    print("=== Qwen3-4B æœ¬åœ°éƒ¨ç½²æµ‹è¯• ===")
    
    # æ£€æŸ¥CUDAå¯ç”¨æ€§
    print(f"CUDA å¯ç”¨: {torch.cuda.is_available()}")
    if torch.cuda.is_available():
        print(f"CUDA è®¾å¤‡æ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰CUDAè®¾å¤‡: {torch.cuda.current_device()}")
        print(f"è®¾å¤‡åç§°: {torch.cuda.get_device_name()}")
    
    # æ¨¡å‹è·¯å¾„
    model_path = "c:/Users/91954/Desktop/ä¸ªäºº/è¯¾ç¨‹/robot_nav/final_project/Qwen3-main/Qwen3-models"
    
    print(f"\nåŠ è½½æ¨¡å‹è·¯å¾„: {model_path}")
    
    try:
        # åŠ è½½tokenizer
        print("æ­£åœ¨åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
        print("âœ“ Tokenizer åŠ è½½æˆåŠŸ")
        
        # åŠ è½½æ¨¡å‹
        print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
        device = "cuda" if torch.cuda.is_available() else "cpu"
        model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            trust_remote_code=True
        )
        print(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼Œè¿è¡Œè®¾å¤‡: {device}")
        
        # æµ‹è¯•æ¨ç†
        print("\nå¼€å§‹æµ‹è¯•æ¨ç†...")
        test_prompt = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
        
        # ç¼–ç è¾“å…¥
        inputs = tokenizer(test_prompt, return_tensors="pt")
        if device == "cuda":
            inputs = {k: v.cuda() for k, v in inputs.items()}
        
        # ç”Ÿæˆå›å¤
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # è§£ç è¾“å‡º
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print(f"\næµ‹è¯•è¾“å…¥: {test_prompt}")
        print(f"æ¨¡å‹å›å¤: {response[len(test_prompt):].strip()}")
        
        print("\nâœ“ Qwen3-4B æœ¬åœ°éƒ¨ç½²æµ‹è¯•æˆåŠŸï¼")
        return True
        
    except Exception as e:
        print(f"\nâœ— æ¨¡å‹éƒ¨ç½²æµ‹è¯•å¤±è´¥: {str(e)}")
        return False

if __name__ == "__main__":
    success = test_qwen3_deployment()
    if success:
        print("\nğŸ‰ Qwen3-4B å·²æˆåŠŸéƒ¨ç½²å¹¶å¯ä»¥æ­£å¸¸ä½¿ç”¨ï¼")
    else:
        print("\nâŒ Qwen3-4B éƒ¨ç½²å­˜åœ¨é—®é¢˜ï¼Œè¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å’Œä¾èµ–ã€‚")